{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "\n",
    "sys.path.insert(0, '../../src/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from banglanlp.text.WordProcessor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnp = BanglaWordProcessor()\n",
    "enp = EnglishWordProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../../data/transliteration/train.tsv', sep='\\t', header=None)\n",
    "df2 = pd.read_csv('../../data/transliteration/validation.tsv', sep='\\t', header=None)\n",
    "df3 = pd.read_csv('../../data/transliteration/dictionary_data/Dictionary_data_cleaned.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3496, 2), (504, 2), (16209, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape, df2.shape, df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['beng','eng']\n",
    "df1.columns = columns\n",
    "df2.columns = columns\n",
    "df3.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(word, lang):\n",
    "    if lang is 'en':\n",
    "        return enp.process(word)\n",
    "    else:\n",
    "        return bnp.process(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['clean_beng'] = df1['beng'].apply(clean, lang='bn')\n",
    "df1['clean_eng'] = df1['eng'].apply(clean, lang='en')\n",
    "df2['clean_beng'] = df2['beng'].apply(clean, lang='bn')\n",
    "df2['clean_eng'] = df2['eng'].apply(clean, lang='en')\n",
    "df3['clean_beng'] = df3['beng'].apply(clean, lang='bn')\n",
    "df3['clean_eng'] = df3['eng'].apply(clean, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop_duplicates(inplace=True)\n",
    "df2.drop_duplicates(inplace=True)\n",
    "df3.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dropna(inplace=True)\n",
    "df2.dropna(inplace=True)\n",
    "df3.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beng</th>\n",
       "      <th>eng</th>\n",
       "      <th>clean_beng</th>\n",
       "      <th>clean_eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [beng, eng, clean_beng, clean_eng]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[df1['clean_beng'].isna() == True]\n",
    "df1[df1['clean_eng'].isna() == True]\n",
    "df2[df2['clean_beng'].isna() == True]\n",
    "df2[df2['clean_eng'].isna() == True]\n",
    "df3[df3['clean_beng'].isna() == True]\n",
    "df3[df3['clean_eng'].isna() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3464, 4), (504, 4), (11489, 4))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape, df2.shape, df3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation For Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df1[['clean_beng','clean_eng']], df3[['clean_beng','clean_eng']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14953, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO : move everything to library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = 'aeiou'\n",
    "semivowels = 'wy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def tokenize(self, word):\n",
    "        tokens = []\n",
    "        for char in word:\n",
    "            tokens.append(char)\n",
    "        return tokens\n",
    "\n",
    "class BanglaCharTokenizer(Tokenizer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def tokenize(self, word):\n",
    "        return super().tokenize(word)\n",
    "    \n",
    "class EnglishCharTokenizer(Tokenizer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def tokenize(self, word):\n",
    "        return super().tokenize(word)\n",
    "    \n",
    "class EnglishBanglaCharTokenizer(Tokenizer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def tokenize(self, word):\n",
    "        tokens = []\n",
    "        current_token = ''\n",
    "        for char in word:\n",
    "            if char is 'h' and (current_token != '' and current_token[-1] not in vowels and current_token[-1] not in semivowels):\n",
    "                tokens.append(current_token + 'h')\n",
    "                current_token = ''\n",
    "            else:\n",
    "                if current_token != '':\n",
    "                    tokens.append(current_token)\n",
    "                current_token = char\n",
    "                \n",
    "        if current_token != '':\n",
    "            tokens.append(current_token)\n",
    "                \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bntok = BanglaCharTokenizer()\n",
    "entok = EnglishCharTokenizer()\n",
    "ebtok = EnglishBanglaCharTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyBuilder():\n",
    "    '''\n",
    "    This class builds a vocabulary with the tokenizer provided\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, tokenizer):\n",
    "        # Don't use 0 as it can be used as a default value in vector for length normalization\n",
    "        self.tokenToIndex = {'<S>':1, '<E>': 2, '<U>' : 3}\n",
    "        self.indexToToken = {1 : '<S>', 2 : '<E>', 3 : '<U>'}\n",
    "        self.__index__  = 4\n",
    "        \n",
    "        if not isinstance(tokenizer, Tokenizer):\n",
    "            raise AttributeError(\"input param tokenizer is not a Tokenizer instance\")\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def add_to_vocab(self, tokenlist):\n",
    "        '''\n",
    "        Adds a list of tokens into the vocabulary\n",
    "        '''\n",
    "        for token in tokenlist:\n",
    "            if token not in self.tokenToIndex:\n",
    "                self.tokenToIndex[token] = self.__index__ \n",
    "                self.indexToToken[self.__index__] = token\n",
    "                self.__index__ += 1\n",
    "    \n",
    "    def build_vocab(self, series):\n",
    "        '''\n",
    "        Build a vocabulary with series as input\n",
    "        '''\n",
    "        \n",
    "        if self.tokenizer is None:\n",
    "            raise AttributeError(\"Tokenizer is not set.\")\n",
    "            \n",
    "        for word in series.values:\n",
    "            self.add_to_vocab(self.tokenizer.tokenize(word))\n",
    "    \n",
    "    def get_len(self):\n",
    "        return self.__index__\n",
    "    \n",
    "    def text_to_vector(self, text):\n",
    "        vec = [self.tokenToIndex['<S>']]\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        for token in tokens:\n",
    "            if token in self.tokenToIndex:\n",
    "                vec.append(self.tokenToIndex[token])\n",
    "            else:\n",
    "                vec.append(self.tokensToIndex['<U>'])\n",
    "        vec.append(self.tokenToIndex['<E>'])\n",
    "        return vec\n",
    "    \n",
    "    def vector_to_tokens(self, vector):\n",
    "        tokens = []\n",
    "        for elem in vector:\n",
    "            tokens.append(self.indexToToken[elem])\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnvocab = VocabularyBuilder(bntok)\n",
    "envocab = VocabularyBuilder(ebtok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnvocab.build_vocab(df_train['clean_beng'])\n",
    "envocab.build_vocab(df_train['clean_eng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 11, 37, 6, 10, 2] ['<S>', 'b', 'i', 'h', 'a', 'n', '<E>']\n"
     ]
    }
   ],
   "source": [
    "v = envocab.text_to_vector('bihan')\n",
    "t = envocab.vector_to_tokens(v)\n",
    "print(v,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and test Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Vectorizer():\n",
    "    \n",
    "    def __init__(self, vocab_builder):\n",
    "        self.vocab = vocab_builder\n",
    "        self.max_len = 0\n",
    "    \n",
    "    def vectorize(self, data):\n",
    "        \n",
    "        if type(data) == str:\n",
    "            return np.array(self.vocab.text_to_vector(data), dtype=np.int32)\n",
    "        \n",
    "        if type(data) == pd.core.series.Series:\n",
    "            vecs = data.apply(self.vocab.text_to_vector)\n",
    "            self.max_len = get_max_len(vecs)\n",
    "            vec = np.zeros((data.shape[0],self.max_len), dtype=np.int32)\n",
    "            print(vec.shape)\n",
    "            for i in range(data.shape[0]):\n",
    "                vec[i][0:len(vecs[i])] = vecs[i]\n",
    "            \n",
    "            return vec\n",
    "\n",
    "    \n",
    "# add to utils\n",
    "def get_max_len(vectors):\n",
    "    '''vectors is a np array/list of list'''\n",
    "    max_len = 0\n",
    "    for v in vectors:\n",
    "        max_len = max(len(v), max_len)\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = np.array(df_train['clean_eng'].apply(envocab.text_to_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = get_max_len(vecs)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 4, 5, 6, 7, 8, 9, 8, 10, 4, 11, 12, 11, 2]),\n",
       "       list([1, 13, 14, 6, 10, 15, 16, 6, 2]),\n",
       "       list([1, 13, 9, 11, 17, 11, 14, 2]), ...,\n",
       "       list([1, 37, 5, 6, 18, 8, 11, 10, 6, 2]),\n",
       "       list([1, 37, 9, 6, 13, 6, 7, 6, 9, 6, 10, 6, 2]),\n",
       "       list([1, 37, 9, 16, 13, 6, 12, 4, 6, 10, 11, 2])], dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((df_train['clean_eng'].shape[0],max_len), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][0:len(vecs[0])] = vecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  4,  5,  6,  7,  8,  9,  8, 10,  4, 11, 12, 11,  2,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr = Vectorizer(envocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14953, 27)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-1733c46aac7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_eng'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-61-a12560256558>\u001b[0m in \u001b[0;36mvectorize\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "vr.vectorize(df_train['clean_eng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
